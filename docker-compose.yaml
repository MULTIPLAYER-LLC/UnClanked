name: "unclanked"

services:
  nginx:
    image: nginx:1.28.0
    hostname: tmux.io
    ports:
      - "26060:443"
    volumes:
      - ./app-proxy/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./app-proxy/ssl:/etc/nginx/ssl:ro
    restart: unless-stopped
    depends_on: [unclanked]

  unclanked:
    image: unclanked:latest
    hostname: tmux.io
    environment:
      - API_PORT=3000
      - OLLAMA_HOST=http://ollama:11434
      - IMAGE_HOST=http://img:7860
    build:
      context: .
      dockerfile: app-backend/Dockerfile.backend
    restart: unless-stopped
    depends_on: [ollama]

  # ollama:
  #   image: ollama/ollama:0.12.1
  #   environment:
  #     - OLLAMA_DEBUG=2
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ./data/ollama:/root/.ollama
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: [gpu]

  vllm:
    image: tmux.io/vllm-openai:lb-10_5_2025
    restart: unless-stopped
    stop_signal: SIGKILL
    ports:
      - "11435:8000"
    volumes:
      - ./data/vllm:/root/.cache
    command: >
      --model Orion-zhen/Qwen3-1.7B-AWQ
      --max_model_len=4596
      --gpu-memory-utilization=0.35
      --max-num-seqs=1
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  img:
    image: unclanked-img:latest
    build:
      context: .
      dockerfile: app-img/Dockerfile.img
    ports:
      - "7860:7860"
    volumes:
      - ./data/img/__pycache__:/srv/app-img/src/src/__pycache__
    restart: unless-stopped
    stop_signal: SIGKILL
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

